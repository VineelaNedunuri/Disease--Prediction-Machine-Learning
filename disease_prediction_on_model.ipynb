{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df1= pd.read_csv('../Disease-Prediction-Machine-Learning/data/new_dataset1.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('../Disease-Prediction-Machine-Learning/data/new_dataset2.csv')\n",
    "df2.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - Välja modell\n",
    "Välj 3-5 maskininlärningsmodeller, gärna så olika som möjligt. För varje dataset som vi skapade i uppgift 2.3\n",
    "gör följande:\n",
    "\n",
    "- train|validation|test split\n",
    "- skala datasetet med feature standardization och normalization (de görs inte samtidigt, utan i olika omgångar)\n",
    "- definiera hyperparametrar (param_grids) att testa för varje modell\n",
    "- använda GridSearchCV() och välja lämplig evalueringsmetric\n",
    "- gör prediction på valideringsdata\n",
    "- beräkna och spara evaluation score för ditt valda metric\n",
    "- checka bästa parametrarna för respektive modell\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosen models are:\n",
    "- 1. Logistic Regression\n",
    "- 2. KNN \n",
    "- 3. Decision Tree\n",
    "- 4. Random Forest\n",
    "- 5. Gaussian Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression: \n",
    "###### https://en.wikipedia.org/wiki/Logistic_regression\n",
    "- Logistic regression is a statistical model used for binary classification problems, which means it predicts the probability event taking place based on a set of input features. \n",
    "- Based on data it predicts that a patient having a disease or not based on features such as age, gender, blood pressure,BMI etc.Logistic function has a relationship between the input features and the output variable.\n",
    "- It is a widely used algorithm in various fields such as medicine, social sciences and economics.\n",
    "\n",
    "### 2. K-Nearest Neighbors (KNN):\n",
    "###### https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "- KNN  is a non-parametric supervised machine learning algorithm used for both classification and regression tasks.\n",
    "- In this data, KNN can be used for predicting whether a patient has cardiovascular disease or not based on their health metrics.\n",
    "- It identifs the k closest data points (neighbors) in the training set to the given input data point. The algorithm then assigns the output value of the input data point based on the majority class of the k-nearest neighbors.\n",
    "- It is a widely used algorithm in various fields such as medicine, facial recognition,finance,text mining and recommendation systems.\n",
    "\n",
    "### 3. Decision Tree:\n",
    "###### https://en.wikipedia.org/wiki/Decision_tree\n",
    "- A decision tree is a supervised machine learning algorithm which is easy to understand, as it creates a tree-like structure that can be interpreted and visualized.\n",
    "- It can handle both categorical and numerical data and can be used for both classification (i.e., predicting a binary outcome such as \"disease\" or \"no disease\") and regression (i.e., predicting a continuous outcome such as blood pressure or cholesterol levels).\n",
    "- It is a widely used algorithm in various fields such as engineering, fraud detection,credit risk analysis and medical diagnosis.\n",
    "\n",
    "### 4. Random Forest:\n",
    "###### https://en.wikipedia.org/wiki/Random_forest\n",
    "- Random forest is a machine learning algorithm used for classification, regression, and other tasks. \n",
    "- It is an ensemble method that creates multiple decision trees and combines their outputs to make a final prediction.\n",
    "- In this algorithm,each tree is trained on a random subset of the original dataset and a random subset of the features, which helps to reduce overfitting and increase the models accuracy. \n",
    "- It is a widely used algorithm in various fields such as medicine, finance,marketing,image and speech recognition.\n",
    "\n",
    "### 5. Gaussian Naive Bayes:\n",
    "###### https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "- Gaussian Naive Bayes is a probabilistic classification algorithm based on Bayes theorem, which describes the probability of a hypothesis based on prior knowledge and new evidence.\n",
    "-  It is a widely used algorithm in various fields such as text classification, spam filtering, image recognitionand medical diagnosis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For first data df1:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train|validation|test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows and columns of first dataframe:',df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing_models import split_data\n",
    "\n",
    "# For df1 data train |val|test split data\n",
    "X_train1, X_val1, X_test1, y_train1, y_val1, y_test1 = split_data(\n",
    "    df=df1, target_col=\"cardio\",test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{X_train1.shape = }\\n{X_val1.shape = }\\n{X_test1.shape = }\\n{y_train1.shape = }\\n{y_val1.shape = }\\n{y_test1.shape = }\\n\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- skala datasetet med feature standardization och normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling is standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing_models import scale_features\n",
    "\n",
    "# standad scaling for dataset 1\n",
    "pipelines = scale_features(scale_type='standard')\n",
    "pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define hyperparameters for choosen models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression hyperparameters\n",
    "log_param_grid = [ {'LR__C': [0.01, 0.1, 1, 10, 100], 'LR__penalty': ['l2'], 'LR__solver': ['lbfgs','newton-cg','sag','saga'],'LR__max_iter': [1000, 5000, 10000]}]\n",
    "\n",
    "# KNN hyperparameters\n",
    "knn_param_grid = [{'KNN__n_neighbors': [3, 5, 7, 9, 11], 'KNN__weights': ['uniform', 'distance']}]\n",
    "\n",
    "# Decision Tree hyperparameters\n",
    "tree_param_grid = [{'DT__max_depth': [5, 10, 20], 'DT__min_samples_split': [2, 5, 10]}]\n",
    "\n",
    "\n",
    "# Random Forest hyperparameters\n",
    "forest_param_grid = [{'RF__n_estimators': [10, 50, 100, 200], 'RF__max_depth': [5, 10, 20], 'RF__min_samples_split': [2, 5, 10]}]\n",
    "\n",
    "\n",
    "# GaussianNB hyperparameters\n",
    "#Gaussian_param_grid = [{'NB__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Logistic Regression': log_param_grid,\n",
    "    'K-Nearest Neighbor': knn_param_grid,\n",
    "    'Decision Tree': tree_param_grid,\n",
    "    'Random Forest': forest_param_grid,\n",
    "    #'Gaussian Naive Bayes': Gaussian_param_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing_models import grid_search, evaluate_classification\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    print('=============================================\\n')\n",
    "    print(f'{model_name:}\\n')\n",
    "    score_file='results/accuracy_scores.txt'\n",
    "    y_pred=grid_search(pipeline, param_grid, X_train1, y_train1, X_val1, y_val1,'dataset1_standard',score_file)\n",
    "    evaluate_classification(model_name,y_val1, y_pred)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling is minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax scaling for dataset 1\n",
    "pipelines= scale_features(scale_type='minmax')\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing_models import grid_search, evaluate_classification\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    print('==============================================\\n')\n",
    "    print(f'{model_name:}\\n')\n",
    "    grid_search(pipeline, param_grid, X_train1, y_train1, X_val1, y_val1,'dataset1_minmax',score_file)\n",
    "    evaluate_classification(model_name,y_val1, y_pred)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For first data df2:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train|validation|test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows and columns of second dataframe:',df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing_models import split_data,scale_features\n",
    "\n",
    "# For df1 data train |val|test split data\n",
    "X_train2, X_val2, X_test2, y_train2, y_val2, y_test2 = split_data(\n",
    "    df=df2, target_col=\"cardio\", test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{X_train2.shape = }\\n{X_val2.shape = }\\n{X_test2.shape = }\\n{y_train2.shape = }\\n{y_val2.shape = }\\n{y_test2.shape = }\\n\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling is standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standad scaling for dataset 2\n",
    "pipelines_stand = scale_features(scale_type='standard')\n",
    "pipelines_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, pipeline in pipelines_stand.items():\n",
    "    param_grid = param_grids[name]\n",
    "    print(' ===============================================\\n')\n",
    "    print(f'{name:}\\n')\n",
    "    grid_search(pipeline, param_grid, X_train2, y_train2, X_val2, y_val2,'dataset2_standard',score_file)\n",
    "    evaluate_classification(name,y_val2, y_pred)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling is minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min max scaling for dataset 2\n",
    "pipelines_max = scale_features(scale_type='minmax')\n",
    "pipelines_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, pipeline in pipelines_max.items():\n",
    "    param_grid = param_grids[name]\n",
    "    print(' ==================================================\\n')\n",
    "    print(f'{name:}\\n')\n",
    "    grid_search(pipeline, param_grid, X_train2, y_train2, X_val2, y_val2,'dataset2_minax',score_file)\n",
    "    evaluate_classification(name,y_val2, y_pred)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vilket dataset väljer du och vilken modell väljer du? Använd den modellen du valt och träna på all data förutom testdatan.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/\n",
    "- Accuracy is a common metric used to measure overall model performance but it can be misleading if the dataset is imbalanced which means one class is much more prevalent than the other. In such cases we can consider precision, recall, and F1 score can be more informative.\n",
    "- So here F1 score and accuracy are consider to measure the model performance.\n",
    "\n",
    "- Based on the results that are saved in accuracy scores  text file, it seems that the Random Forest classifier is the best model because it achieved the highest F1 score and accuracy on both datasets.\n",
    "\n",
    "- While in datasets, it seems that dataset2_standard performed better than dataset1_standard as it achieved higher F1 and accuracy scores across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset2 with standard scale: Random forest classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# pipline and hyperparameters for Random Forest\n",
    "random_pipeline = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), (\"rf\", RandomForestClassifier())]\n",
    ")\n",
    "param_grid_rf = {\n",
    "    \"rf__max_depth\": [10],\n",
    "    \"rf__min_samples_split\": [10],\n",
    "    \"rf__n_estimators\": [100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=random_pipeline,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    error_score=\"raise\",\n",
    ")\n",
    "\n",
    "\n",
    "# to fit that object to training data\n",
    "grid_search.fit(X_train2, y_train2)\n",
    "\n",
    "\n",
    "# predictions on test data\n",
    "y_pred = grid_search.predict(X_test2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Ensemble\n",
    "Använd VotingClassifier() på datasetet som du valt och lägg in de bästa parametrarna för respektive\n",
    "modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the voting classifier for 5 models with their best parameters \n",
    "vote_clf = VotingClassifier(\n",
    "    [\n",
    "        (\"LR\", LogisticRegression(C=100, max_iter=1000, penalty ='l2', solver='saga')),\n",
    "        (\"KNN\", KNeighborsClassifier(n_neighbors = 11, weights='distance')),\n",
    "        (\"DT\", DecisionTreeClassifier(max_depth=10,min_samples_split=2)),\n",
    "        (\"RF\", RandomForestClassifier(max_depth=10,min_samples_split=10,n_estimators=100)),\n",
    "        (\"NB\", GaussianNB(var_smoothing = 1e-09)),\n",
    "    ],\n",
    "    voting=\"hard\",\n",
    ")\n",
    "\n",
    "\n",
    "# Train the voting classifier\n",
    "vote_clf.fit(X_train2, y_train2)\n",
    "\n",
    "# Evaluate the accuracy of the voting classifier on the test set\n",
    "y_pred_clf = vote_clf.predict(X_test2)\n",
    "accuracy = accuracy_score(y_test2, y_pred_clf)\n",
    "print(f'Accuracy of the voting classifier: {accuracy*100:.3f}%')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Evalueringar\n",
    "Gör confusion matrices och classification reports för 2.4 och 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.fit(X_train2, y_train2)\n",
    "    y_pred = model.predict(X_test2)\n",
    "\n",
    "    print(classification_report(y_test2, y_pred))\n",
    "    cm = confusion_matrix(y_test2, y_pred)\n",
    "    ConfusionMatrixDisplay(cm, display_labels=[\"Yes\", \"No\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For daataset 2 with random forest\n",
    "evaluate_model(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for voting classifier\n",
    "evaluate_model(vote_clf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 \"Deploy\" - spara modell\n",
    "Börja med att plocka ut 100 slumpmässigt valda rader från ditt dataset. Exportera dessa 100 samples i\n",
    "test_samples.csv. Därefter tar du den bästa modellen och träna på all data vi har förutom de 100\n",
    "datapunkterna du plockade ut. Spara därefter modellen i en .pkl-fil med hjälp av joblib.dump(). För\n",
    "modellen kan du behöva använda argumentet compress för att komprimera om filstorleken för stor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 100 randomly selected rows from dataset 2\n",
    "random_samples= df2.sample(n=100, random_state=42)\n",
    "random_samples.to_csv('data/test_samples.csv', index=False)\n",
    "\n",
    "\n",
    "# train on remaining data except picked 100 datapoints\n",
    "df = df2.drop(random_samples.index)\n",
    "X = df.drop('cardio', axis=1)\n",
    "y = df['cardio']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-learning-tetaV3aO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
